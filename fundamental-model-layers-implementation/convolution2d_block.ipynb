{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1148e9570>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "np.random.seed(3)\n",
    "torch.manual_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.random.randint(low=0, high=255, size=(28,28,3))\n",
    "filters = np.random.rand(4,3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28, 28, 3), (4, 3, 3, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape, filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the image\n",
    "image = (image - np.mean(image))/np.std(image)\n",
    "\n",
    "# reshape image\n",
    "image = image.reshape(3,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_2d(input_a, input_b):\n",
    "    output = 0.0\n",
    "    size_h, size_w = input_a.shape\n",
    "    for h in range(size_h):\n",
    "        for w in range(size_w):\n",
    "            output+= input_a[h,w]*input_b[h,w]\n",
    "    return output\n",
    "\n",
    "def conv2d(image, filters, padding=0, stride=1):\n",
    "    in_features, image_h, image_w = image.shape\n",
    "    out_features, in_features, kernel_h, kernel_w = filters.shape\n",
    "\n",
    "    out_h, out_w = ((image_h - kernel_h + 2*padding)//stride) + 1, \\\n",
    "                    ((image_w - kernel_w + 2*padding)//stride) + 1\n",
    "    \n",
    "    out_shape = (out_features, out_h, out_w)\n",
    "\n",
    "    out_feature_maps = np.zeros(out_shape)\n",
    "\n",
    "    for b in range(out_features):\n",
    "        # working on feature map i\n",
    "        for c in range(in_features):\n",
    "            # working with kernels present in filter bank b starting with channel c\n",
    "            kernels_channel = filters[b,c] # (3x3)\n",
    "            image_channel = image[c] # (28x28)\n",
    "            for h in range(out_h):\n",
    "                for w in range(out_w):\n",
    "\n",
    "                    sub_image = image_channel[h:h+kernel_h,w:w+kernel_w] # (3x3)\n",
    "\n",
    "                    # add feature map from each channel\n",
    "                    out_feature_maps[b,h,w] += correlate_2d(sub_image, kernels_channel) \n",
    "    \n",
    "    return out_feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 26, 26)\n"
     ]
    }
   ],
   "source": [
    "out_feature_map = conv2d(image=image, filters=filters)\n",
    "print(out_feature_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.Size([4, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_torch = torch.from_numpy(image).unsqueeze(0) #torch.Size([1, 3, 28, 28])\n",
    "filter_torch = torch.from_numpy(filters).double() #torch.Size([1, 4, 3, 3])\n",
    "print(filter_torch.dtype) # torch.float64\n",
    "\n",
    "torch_out_map = F.conv2d(input=img_torch, weight=filter_torch ).squeeze(0) #torch.Size([4, 26, 26])\n",
    "\n",
    "print(torch_out_map.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing my conv2d block result against torch.nn.function conv2d layer result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.8961,  1.2250,  2.7879, -0.4398, -5.0349, -2.1486,  0.7116,  1.7559,\n",
       "        -3.0286,  0.8784, -1.8647,  4.9315,  5.7320,  1.3102,  6.7025,  1.0541,\n",
       "         0.6775,  4.3778,  1.2554, -0.4991, -1.7834, -4.2190, -4.8050, -6.1685,\n",
       "        -2.3139, -0.1779], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_out_map[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.89606706,  1.2250349 ,  2.78789163, -0.43984179, -5.03493435,\n",
       "       -2.14861773,  0.71161341,  1.75591783, -3.02859738,  0.87843483,\n",
       "       -1.86466551,  4.93153718,  5.73203131,  1.31024769,  6.70247558,\n",
       "        1.05413358,  0.67753347,  4.37777351,  1.25540612, -0.49911672,\n",
       "       -1.78337572, -4.2190363 , -4.80495791, -6.16850239, -2.31389806,\n",
       "       -0.17792846])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_feature_map[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_out_feature_map_numpy = torch_out_map.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.030360949215773e-17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(torch_out_feature_map_numpy-out_feature_map) # error between the two feature maps is close to zero!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
