{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Check dropout behavior on a simple NN with the following architecture:\n",
    "- 4 input nodes\n",
    "- hidden layer 1 with 6 nodes + bias\n",
    "- hidden layer 2 with 5 nodes + bias\n",
    "- output layer with 1 node\n",
    "- relu activations used in NN hidden layers\n",
    "ref: # https://nilanjanchattopadhyay.github.io/basics/2020/04/20/Regularization-from-Scratch-Dropout.html\n",
    "\n",
    "'''\n",
    "torch.manual_seed(0)\n",
    "############# TRAINING #############\n",
    "\n",
    "\n",
    "# Mini Batch of 10 elements\n",
    "# Input with 2 dimensions\n",
    "X = torch.rand(10, 4)\n",
    "\n",
    "# Initialize the weights and biases\n",
    "# hidden layer 1\n",
    "W1 = torch.rand(4, 6)\n",
    "B1 = torch.rand(6)\n",
    "\n",
    "# hidden layer 2\n",
    "W2 = torch.rand(6, 5)\n",
    "B2 = torch.rand(5)\n",
    "\n",
    "# output layer\n",
    "W3 = torch.rand(5, 1)\n",
    "B3 = torch.rand(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 1.1927, 1.1133, 1.5882, 0.0000],\n",
      "        [0.0000, 0.0000, 1.9356, 1.6609, 1.8461, 0.0000],\n",
      "        [0.0000, 0.0000, 1.5445, 1.2891, 1.6806, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2012, 1.1219, 1.2317, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4890, 1.2556, 1.7159, 0.0000],\n",
      "        [0.0000, 0.0000, 2.1387, 1.7740, 2.0687, 0.0000],\n",
      "        [0.0000, 0.0000, 1.7402, 1.1175, 1.8941, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6630, 1.5181, 1.5129, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8459, 0.8050, 1.1516, 0.0000],\n",
      "        [0.0000, 0.0000, 2.0426, 1.7400, 2.1815, 0.0000]]) \n",
      " tensor([[2.5590, 3.6239, 2.9962, 0.0000, 0.0000],\n",
      "        [2.9882, 4.7482, 4.0198, 0.0000, 0.0000],\n",
      "        [2.7072, 4.0990, 3.4328, 0.0000, 0.0000],\n",
      "        [2.3193, 3.3671, 2.7001, 0.0000, 0.0000],\n",
      "        [2.7155, 4.0592, 3.4051, 0.0000, 0.0000],\n",
      "        [3.1943, 5.1549, 4.4203, 0.0000, 0.0000],\n",
      "        [2.7884, 4.3728, 3.7417, 0.0000, 0.0000],\n",
      "        [2.6919, 4.1803, 3.4553, 0.0000, 0.0000],\n",
      "        [2.1207, 2.8389, 2.2328, 0.0000, 0.0000],\n",
      "        [3.2539, 5.1348, 4.4232, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''To apply dropout we will create a binary vector, commonly called as binary mask, where 1’s \n",
    "will represent the units to keep and 0’s will represent the units to drop.'''\n",
    "\n",
    "'''Let’s apply dropout to its hidden layers with p=0.6. \n",
    "    p  is the ‘keep probability’. This makes the probability of a hidden unit being dropped equal 1−p=0.4\n",
    "'''\n",
    "\n",
    "########################### TRAINING ###########################\n",
    "\n",
    "H1 = X @ W1 + B1\n",
    "H1.clamp_(0) # relu activation in-place\n",
    "p = 0.6\n",
    "mask = torch.zeros(1,6).bernoulli_(1-p) # dropout on hidden layer 1 with keep prob=0.6\n",
    "H1 *= mask\n",
    "\n",
    "H2 = H1 @ W2 + B2\n",
    "H2.clamp_(0) # relu activation in-place\n",
    "p = 0.5\n",
    "mask = torch.zeros(1,5).bernoulli_(1-p) # dropout on hidden layer 1 with keep prob=0.5\n",
    "H2 *= mask\n",
    "\n",
    "out = H2 @ W3 + B3\n",
    "print(H1, '\\n', H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6977, 0.4997, 0.4771, 0.4453, 0.6353, 0.7849],\n",
      "        [0.7733, 0.6038, 0.7742, 0.6644, 0.7384, 0.9302],\n",
      "        [0.7341, 0.5378, 0.6178, 0.5156, 0.6722, 0.8087],\n",
      "        [0.4835, 0.3474, 0.4805, 0.4488, 0.4927, 0.6098],\n",
      "        [0.7799, 0.5604, 0.5956, 0.5022, 0.6863, 0.8594],\n",
      "        [0.9164, 0.7062, 0.8555, 0.7096, 0.8275, 1.0686],\n",
      "        [0.8468, 0.6108, 0.6961, 0.4470, 0.7577, 0.7054],\n",
      "        [0.6130, 0.4707, 0.6652, 0.6072, 0.6052, 0.8153],\n",
      "        [0.4789, 0.3148, 0.3384, 0.3220, 0.4607, 0.5317],\n",
      "        [0.9045, 0.7260, 0.8170, 0.6960, 0.8726, 1.0088]]) \n",
      " tensor([[1.1470, 1.5303, 1.1804, 1.0582, 0.6567],\n",
      "        [1.2989, 1.8529, 1.4588, 1.2406, 0.8280],\n",
      "        [1.1988, 1.6515, 1.2888, 1.1237, 0.7288],\n",
      "        [0.9997, 1.3338, 0.9887, 0.8613, 0.5247],\n",
      "        [1.2181, 1.6766, 1.3153, 1.1538, 0.7498],\n",
      "        [1.4087, 2.0394, 1.6339, 1.3879, 0.9481],\n",
      "        [1.2468, 1.7073, 1.3610, 1.1951, 0.7939],\n",
      "        [1.1577, 1.6309, 1.2510, 1.0591, 0.6899],\n",
      "        [0.9376, 1.1884, 0.8689, 0.7910, 0.4560],\n",
      "        [1.4240, 2.0170, 1.6129, 1.3953, 0.9253]])\n"
     ]
    }
   ],
   "source": [
    "########################### INFERENCE ###########################\n",
    "H1 = X @ W1 + B1\n",
    "H1.clamp_(0) # relu activation in-place\n",
    "p = 0.6\n",
    "# Scaling the output of Hidden Layer 1\n",
    "H1 *= (1-p)\n",
    "\n",
    "H2 = H1 @ W2 + B2\n",
    "H2.clamp_(0) # relu activation in-place\n",
    "# Scaling the output of Hidden Layer 2\n",
    "p = 0.5\n",
    "H2 *= (1-p)\n",
    "\n",
    "out = H2 @ W3 + B3\n",
    "print(H1, '\\n', H2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3605, 0.0000, 0.0000, 0.0000, 3.9705, 0.0000],\n",
      "        [4.8330, 0.0000, 0.0000, 0.0000, 4.6152, 0.0000],\n",
      "        [4.5881, 0.0000, 0.0000, 0.0000, 4.2014, 0.0000],\n",
      "        [3.0219, 0.0000, 0.0000, 0.0000, 3.0793, 0.0000],\n",
      "        [4.8746, 0.0000, 0.0000, 0.0000, 4.2896, 0.0000],\n",
      "        [5.7276, 0.0000, 0.0000, 0.0000, 5.1718, 0.0000],\n",
      "        [5.2927, 0.0000, 0.0000, 0.0000, 4.7353, 0.0000],\n",
      "        [3.8315, 0.0000, 0.0000, 0.0000, 3.7824, 0.0000],\n",
      "        [2.9930, 0.0000, 0.0000, 0.0000, 2.8791, 0.0000],\n",
      "        [5.6530, 0.0000, 0.0000, 0.0000, 5.4538, 0.0000]]) \n",
      " tensor([[ 0.0000,  0.0000,  0.0000, 14.1767,  8.2431],\n",
      "        [ 0.0000,  0.0000,  0.0000, 15.9266,  9.1336],\n",
      "        [ 0.0000,  0.0000,  0.0000, 14.9103,  8.6712],\n",
      "        [ 0.0000,  0.0000,  0.0000, 10.5048,  5.7306],\n",
      "        [ 0.0000,  0.0000,  0.0000, 15.5553,  9.2079],\n",
      "        [ 0.0000,  0.0000,  0.0000, 18.3270, 10.8122],\n",
      "        [ 0.0000,  0.0000,  0.0000, 16.9322,  9.9944],\n",
      "        [ 0.0000,  0.0000,  0.0000, 12.9510,  7.2519],\n",
      "        [ 0.0000,  0.0000,  0.0000, 10.1769,  5.6746],\n",
      "        [ 0.0000,  0.0000,  0.0000, 18.5782, 10.6757]])\n"
     ]
    }
   ],
   "source": [
    "########################### TRAINING ###########################\n",
    "\n",
    "H1 = X @ W1 + B1\n",
    "H1.clamp_(0) # relu activation in-place\n",
    "p = 0.6\n",
    "mask = torch.zeros(1,6).bernoulli_(1-p) # dropout on hidden layer 1 with keep prob=0.6\n",
    "H1 *= mask\n",
    "# Scaling the output of Hidden Layer 1\n",
    "H1 /= (1-p)\n",
    "\n",
    "H2 = H1 @ W2 + B2\n",
    "H2.clamp_(0) # relu activation in-place\n",
    "p = 0.5\n",
    "mask = torch.zeros(1,5).bernoulli_(1-p) # dropout on hidden layer 1 with keep prob=0.5\n",
    "H2 *= mask\n",
    "# Scaling the output of Hidden Layer 2\n",
    "H2 /= (1-p)\n",
    "\n",
    "out = H2 @ W3 + B3\n",
    "print(H1, '\\n', H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
