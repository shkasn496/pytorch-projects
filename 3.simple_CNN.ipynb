{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper code from Training session for FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn # all neural network modules, nn.Linear, nn.Conv2d, BatchNorm, loss functions\n",
    "import torch.optim as optim # all optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F # all functions that dont have any parameters eg: activations like relu\n",
    "from torch.utils.data import DataLoader # gives easier dataset management and creates mini batches\n",
    "import torchvision.datasets as datasets # standard public datasets \n",
    "import torchvision.transforms as transforms # transforms on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Convolutional Neural Network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, inchannels, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        # formula for conv output\n",
    "            #  out_x = floor((in_x + 2*p_x - K_x)/s_x) + 1\n",
    "                # where in_x=input_features in x dim\n",
    "                # p_x=padding x dim, K_x=kernel size x dim, s_x=stride x dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=inchannels, out_channels=8,\n",
    "                               kernel_size=(3,3), stride=(1,1), padding=(1,1)) # out=8x28x28\n",
    "        # formula for pool output\n",
    "        # n_pool_x = floor( (in_x - k_x)/s_x) + 1 )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)) # will half the conv layer size # out=8x14x14\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=self.conv1.out_channels, out_channels=16,\n",
    "                               kernel_size=(3,3), stride=(1,1), padding=(1,1)) # out=16x14x14\n",
    "        # need to consider inchannels from previous conv and pooling operation\n",
    "        # since pooling op divided features by half bcoz of kernel=(2,2) and stride=(2,2)\n",
    "        # multiply infeatures of linear layer with out_channels or previous layer and pool channels = 14/2=7\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=self.conv2.out_channels * 7 * 7, out_features=7 * 7)\n",
    "        self.fc2 = nn.Linear(in_features=self.fc1.out_features, out_features=num_classes)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # torch.Size([64, 8, 28, 28])\n",
    "        x = self.pool(x) # torch.Size([64, 8, 14, 14])\n",
    "        x = F.relu(self.conv2(x)) # torch.Size([64, 16, 14, 14])\n",
    "        x = self.pool(x) # torch.Size([64, 16, 7, 7])\n",
    "\n",
    "        # need to flatten conv features from NxCxHxW to Nxfeatures before sending to fc block\n",
    "        x = x.view(x.shape[0],-1)  # torch.Size([64, 784])\n",
    "\n",
    "        x = F.relu(self.fc1(x)) # torch.Size([64, 49])\n",
    "        x = self.fc2(x) # returns logits = (batch_size, num_classes) torch.Size([64, 10])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the CNN architecture on dummy data\n",
    "model = CNN(inchannels=1, num_classes=10)\n",
    "x = torch.rand(64, 1, 28, 28)\n",
    "model(x).shape #torch.Size([64, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Hyperparameters\n",
    "inchannels = 1\n",
    "num_classes = 10\n",
    "lr = 0.001\n",
    "batch_size=64\n",
    "num_epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load Data (Simple MNIST)\n",
    "train_dataset=datasets.MNIST(root='dataset/', train=True, \n",
    "                             transform=transforms.ToTensor(),\n",
    "                             download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_dataset=datasets.MNIST(root='dataset/', train=False, \n",
    "                             transform=transforms.ToTensor(),\n",
    "                             download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                          shuffle=False)\n",
    "\n",
    "\n",
    "n_iterations = math.ceil(len(train_dataset)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Initialize network\n",
    "model = CNN(inchannels==inchannels, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Batch_size/Iterations: 0/938, Train Loss: 2.3005\n",
      "Epoch : 0 Batch_size/Iterations: 300/938, Train Loss: 0.3333\n",
      "Epoch : 0 Batch_size/Iterations: 600/938, Train Loss: 0.1458\n",
      "Epoch : 0 Batch_size/Iterations: 900/938, Train Loss: 0.1433\n",
      "Epoch : 2 Batch_size/Iterations: 0/938, Train Loss: 0.0855\n",
      "Epoch : 2 Batch_size/Iterations: 300/938, Train Loss: 0.0371\n",
      "Epoch : 2 Batch_size/Iterations: 600/938, Train Loss: 0.0639\n",
      "Epoch : 2 Batch_size/Iterations: 900/938, Train Loss: 0.0131\n",
      "Epoch : 4 Batch_size/Iterations: 0/938, Train Loss: 0.0104\n",
      "Epoch : 4 Batch_size/Iterations: 300/938, Train Loss: 0.1327\n",
      "Epoch : 4 Batch_size/Iterations: 600/938, Train Loss: 0.0632\n",
      "Epoch : 4 Batch_size/Iterations: 900/938, Train Loss: 0.0720\n",
      "Epoch : 6 Batch_size/Iterations: 0/938, Train Loss: 0.0729\n",
      "Epoch : 6 Batch_size/Iterations: 300/938, Train Loss: 0.0606\n",
      "Epoch : 6 Batch_size/Iterations: 600/938, Train Loss: 0.1181\n",
      "Epoch : 6 Batch_size/Iterations: 900/938, Train Loss: 0.0759\n",
      "Epoch : 8 Batch_size/Iterations: 0/938, Train Loss: 0.1449\n",
      "Epoch : 8 Batch_size/Iterations: 300/938, Train Loss: 0.0306\n",
      "Epoch : 8 Batch_size/Iterations: 600/938, Train Loss: 0.0292\n",
      "Epoch : 8 Batch_size/Iterations: 900/938, Train Loss: 0.0127\n"
     ]
    }
   ],
   "source": [
    "# 8. Train Network\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device) # torch.Size([64, 1, 28, 28]), torch.Size([64])\n",
    "\n",
    "        # clean past gradients collected during backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass - compute predictions\n",
    "        logits = model(data)\n",
    "        train_loss = criterion(logits, targets)\n",
    "        \n",
    "        if epoch % 2 == 0 and batch_idx % 300 == 0:\n",
    "            print(f\"Epoch : {epoch} Batch_size/Iterations: {batch_idx}/{n_iterations}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Backward Pass - get the gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Update our weights\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on train data\n",
      "Accuracy achieved : 99.42 on dataset: 60032 and mean loss : 0.019\n",
      "Checking accuracy on test data\n",
      "Accuracy achieved : 98.93 on dataset: 10048 and mean loss : 0.035\n"
     ]
    }
   ],
   "source": [
    "# 9. Check accuracy on training and test to see how good is our model (Eval)\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on train data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")\n",
    "    num_correct=num_samples=0\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            logits=model(data) # (batch_size, num_classes)\n",
    "            \n",
    "            # we need max index in dim=1 which holds num_classes values\n",
    "            prediction_index = torch.argmax(torch.softmax(logits, dim=1), dim=1) # torch.size([64])\n",
    "            num_correct += (prediction_index == targets).sum()\n",
    "            num_samples += prediction_index.shape[0]\n",
    "\n",
    "            val_loss += criterion(logits, targets).item() * data.shape[0] # add avg losses for each batch multiplied by batch size\n",
    "    \n",
    "    acc = (num_correct / num_samples) * 100\n",
    "    print(f\"Accuracy achieved : {acc:.2f} on dataset: {len(loader)*batch_size} and mean loss : {val_loss/len(loader.dataset):.3f}\")\n",
    "\n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
